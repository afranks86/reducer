---
title: "Reducer notes"
author: "Alexander D'Amour and Alexander Franks"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup and Motivation

## Causal effect estimation and overlap

Consider an observational study where we model the data as iid triples $((Y(0), Y(1)), T, X) \sim P$ of potential outcomes, treatment, and covariates, drawn from some superpopulation with distribution $P$. 
Let $P_0(X) := P(X \mid T = 0)$ and $P_1(X) = P(X \mid T = 1)$.
Let $e(X) = P(T = 1 \mid X)$.
Identifcation of the ATE of $T$
$$
\tau^{ATE} := E_P[Y(1) - Y(0)]
$$
depends on two key assumptions.
$$
\begin{align}
(Y(0), Y(1)) \perp T \mid X \quad \text{and} \quad 0 < e(X) < 1.
\end{align}
$$
These are the **unconfoundedness** and **overlap** conditions, respectively.
Under these assumptions, $\tau^{ATE}$ is identified by the observable functional
$$
\tau^{ATE} = E_P[E_P[Y^{obs} \mid X, T = 1] - E_P[Y^{obs} | X, T = 0]].
$$
To make uniform inferential guarantees about estimates of $\tau^{ATE}$, it is common to require that $e(X)$ be bounded away from 0 and 1 by some constant $\eta \in (0, 1/2)$:
$$
\eta < e(X) < 1-\eta.
$$
In many cases, particularly when $X$ is high-dimensional, this assumption may not hold for a particular data-generating process $P$.

This motivates finding a reduction of $X$, $d(X)$, such that overlap holds in the reduction $d(X)$:
$$
\eta < e_d(X) := P(T = 1 \mid ) < 1-\eta.
$$
If $d(X)$ satisfies this condition, the following functional is identified: 
$$
\tau^{ATE}_d = E_P[E_P[Y^{obs} \mid d(X), T = 1] - E_P[Y^{obs} \mid d(X), T = 0]].
$$

The bias induced by conditioning on $d(X)$ instead of $X$ is
$$
\tau^{ATE} - \tau_d^{ATE} = E_P\left[\frac{Cov(Y(1), T \mid d(X))}{e(X)} + \frac{Cov(Y(0), T \mid d(X))}{1-e(X)} \right].
$$

Ideally, we would search for a reduction $d(X)$ that induces no bias.
One sufficient condition is that:
$$
Cov(Y(t), T \mid d(X)) = 0 \quad \text{for } t = 0, 1, \text{ w.p. 1}.
$$

Note that, under unconfoundedness, $Cov(Y(t), T \mid X) = 0$ for all $X$.
Letting $m_t(X) := E[Y(t) \mid X]$,
$$
\begin{align}
Cov(Y(t), T \mid d(X)) &= E[ Cov(Y(t), T \mid X) \mid d(X) ] + Cov( E[Y(t) \mid X], E[T \mid X] \mid d(X) )\\
&= Cov( m_t(X), e(X) \mid d(X)).
\end{align}
$$

Thus, a sufficient condition for unbiasedness is that $d(X)$ satisfies
$$
Cov(m_t(X), e(X) \mid d(X)) = 0 \quad \text{for } t = 0, \text{ w.p. }1.
$$

The goal of the reducer is to take an outcome model $m_0(x)$ and a treatment model $e(X)$ and to identify a function $d(X)$ conditional on which the two are orthogonal.

**Remark**:
We could also balance bias and variance, by considering the semiparametric efficiency bound:
$$
V^{eff} = E_P\left[\frac{Var(Y(1) \mid d(X))}{e_d(X)}
+ \frac{Var(Y(0) \mid d(X))}{1-e_d(X)} + (\tau_d(X) - \tau^{ATE})^2\right],
$$
where $\tau_d(X)$ is the conditional average treatment effect given $d(X)$, $E[Y(1) - Y(0) \mid d(X)]$.



# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X \quad \text{(Expectation of control outcome)}\\
e(X) &= \mathbf{\beta}'X \quad \text{(Propensity score)}\\
d(X) &= \gamma'X \quad \text{(Deconfounding score)}
\end{align}
$$
where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  




### Partial Correlation Condition
$$
\begin{align}
\Sigma_{XX} &= Cov(X)\\
\Lambda &= (\alpha \beta \gamma)' \Sigma_{XX} (\alpha \beta \gamma)\\
\Lambda_{YT|R} &= \Lambda_{YT} - \Lambda_{YT;R} \Lambda_{RR}^{-1} \Lambda_{R;YT}\\
& = \left( \begin{array}{rr} \alpha' \Sigma \alpha&\alpha'\Sigma\beta\\
                       \beta' \Sigma \alpha&\beta'\Sigma\beta
           \end{array}
    \right) - \frac{1}{\gamma' \Sigma \gamma}
    \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)
           \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)'
\end{align}
$$

Condition is
$$
\begin{align}
\alpha' \Sigma \beta &= (\alpha' \Sigma \gamma) (\gamma' \Sigma \gamma)^{-1} (\gamma' \Sigma \beta)\\
\end{align}
$$
which implies that 
$$ \alpha' (\Sigma - \Sigma\gamma (\gamma' \Sigma \gamma)^{-1}\gamma'\Sigma)\beta = 0$$

### Special Case: $X$ has identity covariance, $X \in \mathbb R^3$
Special case $\Sigma = I$; $\alpha, \beta, \gamma$ all vectors in $\mathbb R^3$.
$$
(\gamma'\gamma)(\alpha'\beta) = (\gamma'\alpha)(\beta'\gamma)\quad \Rightarrow \quad
\gamma' (I \alpha'\beta - \alpha\beta') \gamma = 0
$$

**Interpretation of condition**:
If $\gamma$, $\alpha$, $\beta$ are all normalized, says that correlation between $\alpha$ and $\beta$ must be smaller than correlation between $\gamma$ and $\alpha$ and $\gamma$ and $\beta$.
In particular $\alpha$ is $\gamma$ that is the least correlated with $\beta$. As a result, we have the following conjecture.

**Conjecture**:
The prognostic score is the overlap maximizing deconfounding score.

This defines a quadric surface of solutions $\gamma$. We can characterize the surface in terms of the eigenvalues of the symmetrized matrix $A$ defined as:
$$
\begin{align}
\tilde A &= I\alpha'\beta - \alpha \beta'\\
A &= \frac{1}{2}(\tilde A + \tilde A')
\end{align}
$$
Note that $\gamma' \tilde A \gamma = 0$ implies $\gamma' A \gamma = 0$.
If $A$ has two positive eigenvalues and one negative eigenvalue, then the surface of solutions is a cone.

**Conjecture**: $\gamma = \alpha$ and $\gamma = \beta$ are also clearly solutions. Thus, the surface should always contain $\alpha$ and $\beta$.

Code to construct and characterize matrix $A$:
```{r}
diagnostic <- FALSE
alpha <- c(1, -1, 0)
beta <- c(1, 0, 1)

redA <- function(alpha, beta){
  Atilde <- diag(sum(alpha*beta), length(alpha)) - (alpha %*% t(beta))
  A <- (Atilde + t(Atilde))/2
}
A <- redA(alpha, beta)
eigs <- eigen(A)
if(diagnostic){
  print(A)
  print(eigs)
}

cone_params <- eigs$values
S <- rotation <- eigs$vectors
```

Strategy to find solutions $\gamma$. In rotated coordinates $x$, $y$, $z$, with $z$ set to the coordinate with negative eigenvalue, WLOG, fix $z = |\lambda_3|^{-1/2}$ and characterize resulting ellipse given by:
$$
\lambda_1 x^2 + \lambda_2 y^2 = 1
$$
We can now parameterize the solutions for $\gamma$ in terms of $t$:
$$
\gamma(t) = S\left( \begin{array}{c} x(t)\\y(t)\\z(t) \end{array}\right)
 = S\left( \begin{array}{c}\lambda_1^{-1/2} \cos(t)\\\lambda_2^{-1/2} \sin(t)\\|\lambda_3|^{-1/2} \end{array}\right)
$$
Where $S'$ is the transpose of the matrix of eigenvectors of $A$.

```{r}
gamma_sol <- function(t, cone_params, S){
  S %*% c(cone_params[1]^(-1/2) * cos(t),
          cone_params[2]^(-1/2) * sin(t),
          abs(cone_params[3])^(-1/2))
}

vnorm <- function(v){
  sqrt(sum(v^2))
}

g1 <- gamma_sol(3*pi/2, cone_params, S)
g2 <- gamma_sol(pi/2, cone_params, S)
g3 <- gamma_sol(pi/3, cone_params, S)

```

Testing:
```{r}
library(mvtnorm)
X <- rmvnorm(1e4, rep(0, 3), diag(1, 3))
m <- X %*% alpha
e <- X %*% beta

for(g in list(g1, g2, g3)){
  s1 <- X %*% g
  print(sprintf("thing: %f", g))
  print(cor(m, s1))
  print(cor(e, s1))
  mr <- lm(m ~ s1-1)$residuals
  er <- lm(e ~ s1-1)$residuals
  plot(mr, er)
  print(cor(mr, er))
}
par(mfrow=c(1,2))
plot(s1, m)
plot(s1, e)

mr <- lm(m ~ s1-1)$residuals
er <- lm(e ~ s1-1)$residuals
plot(mr, er)

plot(seq(0, 2*pi, by=.05)/pi, sapply(seq(0, 2*pi, by=.05), function(t) cor(X %*% gamma_sol(t, cone_params, S), e)^2), type='l')

that <- optimize(function(t) cor(X %*% gamma_sol(t, cone_params, S), e)^2,
                             c(0, 2*pi))$minimum
ghat <- gamma_sol(that, cone_params, S)
shat <- X %*% ghat
print(that/pi)
print(cor(m,e))
print(cor(shat,e))
print(cor(shat,m))
```

### General solution space for $\gamma$

Assume welog that $\Sigma = I$ and $\gamma$, $\alpha$ and $\beta$ are semi-orthogonal matrices.  Then the above condition is equivalent to
$$ \alpha'(I - \gamma\gamma^T)\beta = 0 $$

Let $M = (I - \gamma\gamma^T)$.  Then $\alpha M\beta' = 0$ implies that $\alpha M$ is in the null space of $\beta$ **or** $M\beta'$ is in the nullspace of $\alpha$.  For a $k$-dimensional reducer $\gamma$, this implies that $M$ is a rank ($p - k$) projection matrix into a subspace of the null spaces of $\alpha$ and/or $\beta$.

Consider an example with $p=5$ and $k=1$.  If $\alpha$ and $\beta$ are both in $R^1$ then their null spaces span a three dimensional subspace of $R^4$.  If $M = N_\alpha = (I - \alpha\alpha')$ or $M = N_\beta = (I - \beta\beta')$ then clearly $\gamma=\alpha$ or $\gamma=\beta$ are solutions.  

<!-- More generally, we can write as $M = N_\alpha \bigoplus N_\beta$.   -->

The interesting solutions are the ones in which $M$ is a projection into a subspace of the nullspaces of both $\alpha$ *and* $\beta$.  For example, we can choose $M$ so that for any $s < p-1$, $s$ dimensions are in the span of the nullspace of $\alpha$ and the $p-s-1$ dimension are in the nullspace of $\beta$.

The most confusing part about this is the the repeated use of orthogonal complements.  First we work in the null space of alpha and beta, define a new subspace based on those, and then take the orthogonal complement to identify the subspace spanned for d(X).  The steps in this method, which yield a valid solution are:

1. Identify the null space of $\alpha$, $N_\alpha$
    + select a random set of coordinate directions $z$ so that $zz^T$ is a projection matrix into a subspace of $N_\alpha$
2. Identify the null space of $\beta$ outside the nullspace of $\alpha$: $N_{\beta/\alpha}$
    + select a random set of coordinate directions $w$ so that $ww^T$ is a projection into a subspace of $N_{\beta/\alpha}$
3. Define $M$ to be the projection matrix into $span(ww^T + zz^T)$ .  If $M$ is rank $p-k$ then $d(X)$ will be $k$-dimensional.
4. Let $g$ be the first $k$ eigenvectors of $I - M$
5. $d(X) = g'X$

I think any solution subspace can be represented in terms of the above procedure but haven't figured out a good way to  parameterize to control the correlation of $d(X)$ with $\alpha$ and $\beta$.

In this SNOTE we assume that $m_0$, $e$ and $d$ are all 1 dimensional.  

```{r}

p <- 12

alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta

# null space of alpha
A <- diag(p) - alpha %*% t(alpha)
# null space of beta
B <- diag(p) - beta %*% t(beta)

AnotB  <- eigen(A %*% (diag(p) - B))$vectors[, 1]
BnotA  <- eigen(B %*% (diag(p) - A))$vectors[, 1]

## this makes d perfectly correlated with alpha
#z <- cbind(AnotB, eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1:(p/2-1)])


## this makes d perfectly correlated with beta
#z <- eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1:p/2]

# A is a rank p-1 matrix, choose p/2 dimensional subspace of A
# choice p/2 is arbitrary but should balance alpha/beta contribution 
# this is step 1
z <- eigen(A)$vectors[, 1:(p-1)] %*% rustiefel(p-1, p/2)

# choose (p/2 - 1) dimensional subspace of B\A
# this is step 2
w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:(p/2-1)]

# Find the projection into the subspace spanned by z and w (step 3)
u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-1)]
M <- u %*% t(u)


## verify zero condition holds
alpha %*% M %*% beta
alpha %*% M %*% M %*% beta

## gamma is now the eigenvector of I - M (step 4)
eig <- eigen(diag(p) - M)

# should be exactly one eigenvalue equal to 1 (rest are 0)
eig$values

g <- eig$vectors[, 1]

## step 5
d <- Re(X %*% g)

print(sprintf("Corr(d, m) = %f", cor(m, d)))
print(sprintf("Corr(d, e) = %f", cor(e, d)))

mr <- lm(m ~ d-1)$residuals
er <- lm(e ~ d-1)$residuals
print(sprintf("Corr(m, e | d) = %f", cor(mr, er)))


```

### Exploring parameterization in terms of alpha/beta similarity with d(x)

TODO: Writeup discription and understand functional for the curve below

```{r}
p <- 12

alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta

# null space of alpha
A <- diag(p) - alpha %*% t(alpha)
# null space of beta
B <- diag(p) - beta %*% t(beta)

AnotB  <- eigen(A %*% (diag(p) - B))$vectors[, 1]
BnotA  <- eigen(B %*% (diag(p) - A))$vectors[, 1]

n <- 100
cor_m <- cor_e <- numeric(n)
i <- 1
for(theta in seq(0, pi/2, length.out=n)) {

  z <- cbind(AnotB, eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1]) %*% c(cos(theta), sin(theta))
  w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:(p-2)]

  u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-1)]
  M <- u %*% t(u)

  ## gamma is now the eigenvector of I - M (step 4)
  eig <- eigen(diag(p) - M)

  g <- eig$vectors[, 1]

  ## step 5
  d <- Re(X %*% g)

  cor_m[i] <- cor(m, d)
  cor_e[i] <- cor(e, d)
  i <- i+1
}

plot(seq(0, pi/2, length.out=n), abs(cor_m), pch=19, col="red", 
     type="l", lwd=3, ylim=c(0, 1), ylab="Correlation with d(X)", xlab="Alpha angle")
lines(seq(0, pi/2, length.out=n), abs(cor_e), pch=19, col="blue", lwd=3)
abline(h=cor(e, m), lty=2)
legend("bottomleft", legend=c("m(X)", "e(X)", "Cor(m,e)"), col=c("red", "blue", "black"), lty=c(1,1, 2))

```


### Example where the dimension of $\alpha$ is greater than 1

WARNING: INCOMPLETE

In this example we assume 200 dimensional covariate space, $X$, and assume a high dimensional response surface $k=30$. 

- Look at example where dim($\gamma$) > 1
- Mixing 
- General reducer is highly correlated with the propensity score


```{r}
p <- 200
k <- 30

## choose a random value for alpha p x k
alpha <- rustiefel(p, k)

eps <- rnorm(p)
eps <- eps/sqrt(sum(eps^2))

## beta is 1d propensity score
beta <- (alpha %*% rustiefel(k, 1)) + eps
beta <- beta/sqrt(sum(beta^2))


library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta
A <- diag(p) - alpha %*% t(alpha)
B <- diag(p) - beta %*% t(beta)

## subspace for the nullspace of alpha and beta

## what are teh differences between these two?
z <- eigen(A)$vectors[, 1:((p-k)/2)]
w <- eigen(B - z %*% t(z))$vectors[, 1:((p-k)/2)]

u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-k)]
M <- u %*% t(u)

## verify zero condition holds
t(alpha) %*% M %*% beta

## gamma is now the eigenvector of I - M
eig <- eigen(diag(p) - M)
g <- eig$vectors[, 1:k]

# should be exactly one eigenvalue equal to 1 (rest are 0)
print(head(eig$values))

## verify
d <- X %*% g

## print(summary(cor(m, d)))
## print(summary(cor(m, e)))

## reducer is highly correlated with propensity score
## print(cor(e, d))
## print(cor(m, d))

print(cor(lm(e ~ m-1)$fitted.values, e))
print(cor(lm(e ~ m-1)$fitted.values, d))
mr <- lm(m ~ d-1)$residuals
er <- lm(e ~ d-1)$residuals

plot(mr[, 1], er)
plot(mr[, 5], er)

print(summary(cor(mr, er)))
summary(apply(mr, 2, function(mrCol) var(mrCol-er)))

```

