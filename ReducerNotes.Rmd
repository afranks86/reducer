---
title: "Reducer notes"
author: "Alexander D'Amour and Alexander Franks"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup and Motivation

## Causal effect estimation and overlap

Consider an observational study where we model the data as iid triples $((Y(0), Y(1)), T, X) \sim P$ of potential outcomes, treatment, and covariates, drawn from some superpopulation with distribution $P$. 
Let $P_0(X) := P(X \mid T = 0)$ and $P_1(X) = P(X \mid T = 1)$.
Let $e(X) = P(T = 1 \mid X)$.
Identifcation of the ATE of $T$
$$
\tau^{ATE} := E_P[Y(1) - Y(0)]
$$
depends on two key assumptions.
$$
\begin{align}
(Y(0), Y(1)) \perp T \mid X \quad \text{and} \quad 0 < e(X) < 1.
\end{align}
$$
These are the **unconfoundedness** and **overlap** conditions, respectively.
Under these assumptions, $\tau^{ATE}$ is identified by the observable functional
$$
\tau^{ATE} = E_P[E_P[Y^{obs} \mid X, T = 1] - E_P[Y^{obs} | X, T = 0]].
$$
To make uniform inferential guarantees about estimates of $\tau^{ATE}$, it is common to require that $e(X)$ be bounded away from 0 and 1 by some constant $\eta \in (0, 1/2)$:
$$
\eta < e(X) < 1-\eta.
$$
In many cases, particularly when $X$ is high-dimensional, this assumption may not hold for a particular data-generating process $P$.

This motivates finding a reduction of $X$, $d(X)$, such that overlap holds in the reduction $d(X)$:
$$
\eta < e_d(X) := P(T = 1 \mid ) < 1-\eta.
$$
If $d(X)$ satisfies this condition, the following functional is identified: 
$$
\tau^{ATE}_d = E_P[E_P[Y^{obs} \mid d(X), T = 1] - E_P[Y^{obs} \mid d(X), T = 0]].
$$

The bias induced by conditioning on $d(X)$ instead of $X$ is
$$
\tau^{ATE} - \tau_d^{ATE} = E_P\left[\frac{Cov(Y(1), X \mid d(X))}{e(X)} + \frac{Cov(Y(0), X \mid d(X))}{1-e(X)} \right].
$$

Ideally, we would search for a reduction $d(X)$ that induces no bias.
One sufficient condition is that:
$$
Cov(Y(t), T \mid d(X)) = 0 \quad \text{for } t = 0, 1, \text{ w.p. 1}.
$$

Note that, under unconfoundedness, $Cov(Y(t), T \mid X) = 0$ for all $X$.
Letting $m_t(X) := E[Y(t) \mid X]$,
$$
\begin{align}
Cov(Y(t), T \mid d(X)) &= E[ Cov(Y(t), T \mid X) \mid d(X) ] + Cov( E[Y(t) \mid X], E[T \mid X] \mid d(X) )\\
&= Cov( m_t(X), e(X) \mid d(X)).
\end{align}
$$

Thus, a sufficient condition for unbiasedness is that $d(X)$ satisfies
$$
Cov(m_t(X), e(X) \mid d(X)) = 0 \quad \text{for } t = 0, \text{ w.p. }1.
$$

The goal of the reducer is to take an outcome model $m_0(x)$ and a treatment model $e(X)$ and to identify a function $d(X)$ conditional on which the two are orthogonal.

**Remark**:
We could also balance bias and variance, by considering the semiparametric efficiency bound:
$$
V^{eff} = E_P\left[\frac{Var(Y(1) \mid d(X))}{e_d(X)}
+ \frac{Var(Y(0) \mid d(X))}{1-e_d(X)} + (\tau_d(X) - \tau^{ATE})^2\right],
$$
where $\tau_d(X)$ is the conditional average treatment effect given $d(X)$, $E[Y(1) - Y(0) \mid d(X)]$.



# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X \quad \text{(Expectation of control outcome)}\\
e(X) &= \mathbf{\beta}'X \quad \text{(Propensity score)}\\
d(X) &= \gamma'X \quad \text{(Deconfounding score)}
\end{align}
$$
where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  


### Partial Correlation Condition
$$
\begin{align}
\Sigma_{XX} &= Cov(X)\\
\Lambda &= (\alpha \beta \gamma)' \Sigma_{XX} (\alpha \beta \gamma)\\
\Lambda_{YT|R} &= \Lambda_{YT} - \Lambda_{YT;R} \Lambda_{RR}^{-1} \Lambda_{R;YT}\\
& = \left( \begin{array}{rr} \alpha' \Sigma \alpha&\alpha'\Sigma\beta\\
                       \beta' \Sigma \alpha&\beta'\Sigma\beta
           \end{array}
    \right) - \frac{1}{\gamma' \Sigma \gamma}
    \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)
           \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)'
\end{align}
$$

Condition is
$$
\begin{align}
\alpha' \Sigma \beta &= (\alpha' \Sigma \gamma) (\gamma' \Sigma \gamma)^{-1} (\gamma' \Sigma \beta)\\
\end{align}
$$
which implies that 
$$ \alpha' (\Sigma - \Sigma\gamma (\gamma' \Sigma \gamma)^{-1}\gamma'\Sigma)\beta = 0$$


### Solution space for $\gamma$

Assume welog that $\Sigma = I$ and $\gamma$, $\alpha$ and $\beta$ are semi-orthogonal matrices.  Then the above condition is equivalent to
$$ \alpha'(I - \gamma\gamma^T)\beta = 0 $$

Let $M = (I - \gamma\gamma^T)$.  Then $\alpha M\beta' = 0$ implies that $\alpha M$ is in the null space of $\beta$ **or** $M\beta'$ is in the nullspace of $\alpha$.  For a $k$-dimensional reducer $\gamma$, this implies that $M$ is a rank ($p - k$) projection matrix into a subspace of the null spaces of $\alpha$ and/or $\beta$.

Consider an example with $p=4$ and $k=1$.  If $\alpha$ and $\beta$ are both in $R^1$ then their null spaces span a three dimensional subspace of $R^4$.  If $M = (I - \alpha\alpha')$ or $M = (I - \beta\beta')$ then clearly the solution is $\alpha$ or $\beta$.  The interesting solutions are the ones in which $M$ is a projection into a subspace of the nullspaces of both $\alpha$ *and* $\beta$.  For example, we can choose $M$ so that 2 dimensions are in the span of the nullspace of $\alpha$ and the third dimension is in the nullspace of $\beta$ (or vice versa).

```{r}

p <- 5
alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta
A <- diag(p) - alpha %*% t(alpha)
B <- diag(p) - beta %*% t(beta)

# choose a random plane in the nullspace of alpha 
mixer <- rustiefel(p-1, 2)
z <- eigen(A)$vectors[, 1:(p-1)] %*% mixer

# choose the direction in the nullspace of beta orthogonal
# To z
w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:2]

## M is now a rank (p-1) projection matrix
M <- z %*% t(z) + w %*% t(w)

## verify zero condition holds
alpha %*% M %*% beta

## gamma is now the eigenvector of I - M
eig <- eigen(diag(p) - M)
g <- eig$vectors[, 1]

# should be exactly one eigenvalue equal to 1 (rest are 0)
eig$values

## verify
s1 <- X %*% g

print(cor(m, s1))
print(cor(e, s1))
print(cor(m, e))
mr <- lm(m ~ s1-1)$residuals
er <- lm(e ~ s1-1)$residuals

plot(mr, er)

print(cor(mr, er))
```

### Example where dimension of $\alpha$ is larger
```{r}
p <- 200
k <- 30

alpha <- rustiefel(p, k)
eps <- rnorm(p)
eps <- eps/sqrt(sum(eps^2))
beta <- (alpha %*% rustiefel(k, 1)) + eps
beta <- beta/sqrt(sum(beta^2))


library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta
A <- diag(p) - alpha %*% t(alpha)
B <- diag(p) - beta %*% t(beta)

## vectors in null space of alpha and beta
z <- eigen(A)$vectors[, 1:(p-k)]
w <- eigen(B - z %*% t(z))$vectors[, 1:(k-1)]

z <- eigen(A)$vectors[, 1:(p-k)] %*% rustiefel(p-k, 20)
w <- eigen(B - z %*% t(z))$vectors[, 1:(p-ncol(z)-1)]

## M is now a rank (p-1) projection matrix
M <- w %*% t(w) + z %*% t(z)

## verify zero condition holds
t(alpha) %*% M %*% beta

## gamma is now the eigenvector of I - M
eig <- eigen(diag(p) - M)
g <- eig$vectors[, 1]

# should be exactly one eigenvalue equal to 1 (rest are 0)
print(head(eig$values))

## verify
s1 <- X %*% g

print(summary(cor(m, s1)))
print(summary(cor(m, e)))
print(cor(e, s1))
print(cor(lm(e ~ m-1)$fitted.values, e))
print(cor(lm(e ~ m-1)$fitted.values, s1))
mr <- lm(m ~ s1-1)$residuals
er <- lm(e ~ s1-1)$residuals

plot(mr[, 1], er)
plot(mr[, 5], er)

print(summary(cor(mr, er)))

```