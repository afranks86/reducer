---
title: "Franks Reducer notes"
author: "Alexander Franks"
date: "4/23/2017"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X\\
e(X) &= \mathbf{\beta}'X\\
r(X) &= \gamma'X
\end{align}
$$
where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  


### Partial Correlation Condition
$$
\begin{align}
\Sigma_{XX} &= Cov(X)\\
\Lambda &= (\alpha \beta \gamma)' \Sigma_{XX} (\alpha \beta \gamma)\\
\Lambda_{YT|R} &= \Lambda_{YT} - \Lambda_{YT;R} \Lambda_{RR}^{-1} \Lambda_{R;YT}\\
& = \left( \begin{array}{rr} \alpha' \Sigma \alpha&\alpha'\Sigma\beta\\
                       \beta' \Sigma \alpha&\beta'\Sigma\beta
           \end{array}
    \right) - \frac{1}{\gamma' \Sigma \gamma}
    \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)
           \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)'
\end{align}
$$

Condition is
$$
\begin{align}
\alpha' \Sigma \beta &= (\alpha' \Sigma \gamma) (\gamma' \Sigma \gamma)^{-1} (\gamma' \Sigma \beta)\\
\end{align}
$$
which implies that 
$$ \alpha' (\Sigma - \Sigma\gamma (\gamma' \Sigma \gamma)^{-1}\gamma'\Sigma)\beta = 0$$


### Solution space for $\gamma$

Assume welog that $\Sigma = I$ and $\gamma$, $\alpha$ and $\beta$ are semi-orthogonal matrices.  Then the above condition is equivalent to
$$ \alpha'(I - \gamma\gamma^T)\beta = 0 $$

Let $M = (I - \gamma\gamma^T)$.  Then $\alpha M\beta' = 0$ implies that $\alpha M$ is in the null space of $\beta$ **or** $M\beta'$ is in the nullspace of $\alpha$.  For a $k$-dimensional reducer $\gamma$, this implies that $M$ is a rank ($p - k$) projection matrix into a subspace of the null spaces of $\alpha$ and/or $\beta$.

Consider an example with $p=4$ and $k=1$.  If $\alpha$ and $\beta$ are both in $R^1$ then their null spaces span a three dimensional subspace of $R^4$.  If $M = (I - \alpha\alpha')$ or $M = (I - \beta\beta')$ then clearly the solution is $\alpha$ or $\beta$.  The interestingsolutions are the ones in which $M$ is a projection into a subspace of the nullspace of both $\alpha$ *and* a subspace of the nullspace of $\beta$.  Thus, we can choose $M$ so that 2 dimensions are in the span of the nullspace of $\alpha$ and the third dimension is in the nullspace of $\beta$ (or vice versa).

```{r}

p <- 4
alpha <- c(1, -1, 0, 0)/sqrt(2)
beta <- c(1, 0, 1, 0)/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta
A <- diag(p) - alpha %*% t(alpha)
B <- diag(p) - beta %*% t(beta)

# choose a random plane in the nullspace of alpha 
mixer <- rustiefel(3, 2)
z <- eigen(A)$vectors[, 1:3] %*% mixer

# choose the direction in the nullspace of beta orthogonal
# To z
w <- eigen(B - z %*% t(z))$vectors[, 1]

## M is now a rank (p-1) projection matrix
M <- z %*% t(z) + w %*% t(w)

## verify zero condition holds
alpha %*% M %*% beta

## gamma is now the eigenvector of I - M
eig <- eigen(diag(p) - M)
g <- eig$vectors[, 1]

# should be exactly one eigenvalue equal to 1 (rest are 0)
eig$values

## verify
s1 <- X %*% g

print(cor(m, s1))
print(cor(e, s1))
print(cor(m, e))
mr <- lm(m ~ s1-1)$residuals
er <- lm(e ~ s1-1)$residuals

plot(mr, er)

print(cor(mr, er))
```



