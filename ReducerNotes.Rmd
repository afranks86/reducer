---
title: "Reducer notes"
author: "Alexander D'Amour and Alexander Franks"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup and Motivation

## Causal effect estimation and overlap

Consider an observational study where we model the data as iid triples $((Y(0), Y(1)), T, X) \sim P$ of potential outcomes, treatment, and covariates, drawn from some superpopulation with distribution $P$. 
Let $P_0(X) := P(X \mid T = 0)$ and $P_1(X) = P(X \mid T = 1)$.
Let $e(X) = P(T = 1 \mid X)$.
Identifcation of the ATE of $T$
$$
\tau^{ATE} := E_P[Y(1) - Y(0)]
$$
depends on two key assumptions.
$$
\begin{align}
(Y(0), Y(1)) \perp T \mid X \quad \text{and} \quad 0 < e(X) < 1.
\end{align}
$$
These are the **unconfoundedness** and **overlap** conditions, respectively.
Under these assumptions, $\tau^{ATE}$ is identified by the observable functional
$$
\tau^{ATE} = E_P[E_P[Y^{obs} \mid X, T = 1] - E_P[Y^{obs} | X, T = 0]].
$$
To make uniform inferential guarantees about estimates of $\tau^{ATE}$, it is common to require that $e(X)$ be bounded away from 0 and 1 by some constant $\eta \in (0, 1/2)$:
$$
\eta < e(X) < 1-\eta.
$$
In many cases, particularly when $X$ is high-dimensional, this assumption may not hold for a particular data-generating process $P$.

This motivates finding a reduction of $X$, $d(X)$, such that overlap holds in the reduction $d(X)$:
$$
\eta < e_d(X) := P(T = 1 \mid ) < 1-\eta.
$$
If $d(X)$ satisfies this condition, the following functional is identified: 
$$
\tau^{ATE}_d = E_P[E_P[Y^{obs} \mid d(X), T = 1] - E_P[Y^{obs} \mid d(X), T = 0]].
$$

The bias induced by conditioning on $d(X)$ instead of $X$ is
$$
\tau^{ATE} - \tau_d^{ATE} = E_P\left[\frac{Cov(Y(1), T \mid d(X))}{e_d(X)} + \frac{Cov(Y(0), T \mid d(X))}{1-e_d(X)} \right].
$$

Ideally, we would search for a reduction $d(X)$ that induces no bias.
One sufficient condition is that:
$$
Cov(Y(t), T \mid d(X)) = 0 \quad \text{for } t = 0, 1, \text{ w.p. 1}.
$$

Note that, under unconfoundedness, $Cov(Y(t), T \mid X) = 0$ for all $X$.
Letting $m_t(X) := E[Y(t) \mid X]$,
$$
\begin{align}
Cov(Y(t), T \mid d(X)) &= E[ Cov(Y(t), T \mid X) \mid d(X) ] + Cov( E[Y(t) \mid X], E[T \mid X] \mid d(X) )\\
&= Cov( m_t(X), e(X) \mid d(X)).
\end{align}
$$

Thus, a sufficient condition for unbiasedness is that $d(X)$ satisfies
$$
Cov(m_t(X), e(X) \mid d(X)) = 0 \quad \text{for } t = 0, \text{ w.p. }1.
$$

The goal of the reducer is to take an outcome model $m_0(x)$ and a treatment model $e(X)$ and to identify a function $d(X)$ conditional on which the two are orthogonal.

**Remark**:
We could also balance bias and variance, by considering the semiparametric efficiency bound:
$$
V^{eff} = E_P\left[\frac{Var(Y(1) \mid d(X))}{e_d(X)}
+ \frac{Var(Y(0) \mid d(X))}{1-e_d(X)} + (\tau_d(X) - \tau^{ATE})^2\right],
$$
where $\tau_d(X)$ is the conditional average treatment effect given $d(X)$, $E[Y(1) - Y(0) \mid d(X)]$.



# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X \quad \text{(Expectation of control outcome)}\\
e(X) &= \mathbf{\beta}'X \quad \text{(Propensity score)}\\
d(X) &= \gamma'X \quad \text{(Deconfounding score)}
\end{align}
$$
where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  




### Partial Correlation Condition
$$
\begin{align}
\Sigma_{XX} &= Cov(X)\\
\Lambda &= (\alpha \beta \gamma)' \Sigma_{XX} (\alpha \beta \gamma)\\
\Lambda_{YT|R} &= \Lambda_{YT} - \Lambda_{YT;R} \Lambda_{RR}^{-1} \Lambda_{R;YT}\\
& = \left( \begin{array}{rr} \alpha' \Sigma \alpha&\alpha'\Sigma\beta\\
                       \beta' \Sigma \alpha&\beta'\Sigma\beta
           \end{array}
    \right) - \frac{1}{\gamma' \Sigma \gamma}
    \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)
           \left(
          \begin{array}{r} \alpha' \Sigma \gamma\\
                       \beta' \Sigma \gamma
           \end{array}\right)'
\end{align}
$$

Condition is

$$
\begin{align}
\alpha' \Sigma \beta &= (\alpha' \Sigma \gamma) (\gamma' \Sigma \gamma)^{-1} (\gamma' \Sigma \beta)\\
\end{align}
$$
which implies that 
$$ \alpha' (\Sigma - \Sigma\gamma (\gamma' \Sigma \gamma)^{-1}\gamma'\Sigma)\beta = 0$$

### Special Case: $X$ has identity covariance, $X \in \mathbb R^3$
Special case $\Sigma = I$; $\alpha, \beta, \gamma$ all vectors in $\mathbb R^3$.
$$
(\gamma'\gamma)(\alpha'\beta) = (\gamma'\alpha)(\beta'\gamma)\quad \Rightarrow \quad
\gamma' (I \alpha'\beta - \alpha\beta') \gamma = 0
$$

**Interpretation of condition**:

If $\gamma$, $\alpha$, $\beta$ are all normalized, says that correlation between $\alpha$ and $\beta$ must be smaller than correlation between $\gamma$ and $\alpha$ and $\gamma$ and $\beta$.
In particular $\alpha$ is $\gamma$ that is the least correlated with $\beta$. As a result, we have the following conjecture:

**Conjecture**:
The prognostic score is the overlap maximizing deconfounding score.

This defines a quadric surface of solutions $\gamma$. We can characterize the surface in terms of the eigenvalues of the symmetrized matrix $A$ defined as:

$$
\begin{align}
\tilde A &= I\alpha'\beta - \alpha \beta'\\
A &= \frac{1}{2}(\tilde A + \tilde A')
\end{align}
$$
Note that $\gamma' \tilde A \gamma = 0$ implies $\gamma' A \gamma = 0$.
If $A$ has two positive eigenvalues and one negative eigenvalue, then the surface of solutions is a cone.

**Conjecture**: $\gamma = \alpha$ and $\gamma = \beta$ are also clearly solutions. Thus, the surface should always contain $\alpha$ and $\beta$.

Code to construct and characterize matrix $A$:
```{r}
diagnostic <- FALSE
alpha <- c(1, -1, 0)
beta <- c(1, 0, 1)

redA <- function(alpha, beta){
  Atilde <- diag(sum(alpha*beta), length(alpha)) - (alpha %*% t(beta))
  A <- (Atilde + t(Atilde))/2
}
A <- redA(alpha, beta)
eigs <- eigen(A)
if(diagnostic){
  print(A)
  print(eigs)
}

cone_params <- eigs$values
S <- rotation <- eigs$vectors
```

Strategy to find solutions $\gamma$. In rotated coordinates $x$, $y$, $z$, with $z$ set to the coordinate with negative eigenvalue, WLOG, fix $z = |\lambda_3|^{-1/2}$ and characterize resulting ellipse given by:
$$
\lambda_1 x^2 + \lambda_2 y^2 = 1
$$
We can now parameterize the solutions for $\gamma$ in terms of $t$:
$$
\gamma(t) = S\left( \begin{array}{c} x(t)\\y(t)\\z(t) \end{array}\right)
 = S\left( \begin{array}{c}\lambda_1^{-1/2} \cos(t)\\\lambda_2^{-1/2} \sin(t)\\|\lambda_3|^{-1/2} \end{array}\right)
$$
Where $S'$ is the transpose of the matrix of eigenvectors of $A$.

```{r}
gamma_sol <- function(t, cone_params, S){
  S %*% c(cone_params[1]^(-1/2) * cos(t),
          cone_params[2]^(-1/2) * sin(t),
          abs(cone_params[3])^(-1/2))
}

vnorm <- function(v){
  sqrt(sum(v^2))
}

g1 <- gamma_sol(3*pi/2, cone_params, S)
g2 <- gamma_sol(pi/2, cone_params, S)
g3 <- gamma_sol(pi/3, cone_params, S)

```

Testing:
```{r}
library(mvtnorm)
X <- rmvnorm(1e4, rep(0, 3), diag(1, 3))
m <- X %*% alpha
e <- X %*% beta

for(g in list(g1, g2, g3)){
  s1 <- X %*% g
  print(sprintf("thing: %f", g))
  print(cor(m, s1))
  print(cor(e, s1))
  mr <- lm(m ~ s1-1)$residuals
  er <- lm(e ~ s1-1)$residuals
  plot(mr, er)
  print(cor(mr, er))
}
par(mfrow=c(1,2))
plot(s1, m)
plot(s1, e)

mr <- lm(m ~ s1-1)$residuals
er <- lm(e ~ s1-1)$residuals
plot(mr, er)

plot(seq(0, 2*pi, by=.05)/pi, sapply(seq(0, 2*pi, by=.05), function(t) cor(X %*% gamma_sol(t, cone_params, S), e)^2), type='l')

that <- optimize(function(t) cor(X %*% gamma_sol(t, cone_params, S), e)^2,
                             c(0, 2*pi))$minimum
ghat <- gamma_sol(that, cone_params, S)
shat <- X %*% ghat
print(that/pi)
print(cor(m,e))
print(cor(shat,e))
print(cor(shat,m))
```




### Solution in $\mathbb R^p$

Assume welog that $\Sigma = I$ and $\gamma$, $\alpha$ and $\beta$ are vectors in $\mathbb R^p$.  

$$(\alpha'\gamma)(\gamma'\beta) = \alpha'\beta$$

which is equivalent to

$$\gamma^T \left(\frac{\alpha \beta' + \beta \alpha'}{2}\right)\gamma = \alpha'\beta$$

Let $$M = \left(\frac{\alpha \beta' + \beta \alpha'}{2}\right)$$

If $\alpha$ and $\beta$ are not colinear then $M$ is rank 2 with exactly one positive eigenvalue and one negative eigenvalue:

$$\gamma'U\Lambda U\gamma = \alpha'\beta$$
where $U\Lambda U'$ is the eigendecomposition of M.  Specifically, 

$$
\begin{align}
u_1 &= \frac{(\alpha + \beta)}{\sqrt{2 + 2\alpha'\beta}}, u_2 = \frac{(\alpha - \beta)}{\sqrt{2 - 2\alpha'\beta}}\\
\lambda_1 &= \frac{(\alpha'\beta+1)}{2}, \lambda_2 = \frac{(\alpha'\beta-1)}{2}
\end{align}
$$

Then we have a solution characterized by the $p-2$--dimensional hyperbola

$$ \frac{\lambda_1}{\alpha'\beta}w_1 - \frac{\lambda_2}{\alpha'\beta}w_2 = 1$$
for $w_i = \gamma'u_i$ and $\lambda_2$ is the absolute value of the negative eigenvalue in $\Lambda$.

Written a little bit more explicitly:
$$
 \gamma'(\lambda_1 u_1 - \lambda_2 u_2) = \alpha'\beta
$$
Note that this constrains two specific projections of $\gamma$, but leaves all other dimensions free.

A couple things to note:



- $w_2 = 0$ corresponds to a solution in which $\gamma$ is equiangular with $\alpha$ and $\beta$. Such solutions can be found as $\sqrt{\frac{\alpha'\beta}{\lambda_1}}u_1 + \sqrt{(1-\frac{\alpha'\beta}{\lambda_1})}z \text{ for all } z \in Null(U)$
- By change of basis (and because $\alpha$ and $\beta$ are in the span of $u_1$ and $u_2$) we have $\alpha = (\alpha'u_1)u_1 + (\alpha'u_2)u_2$ and $\beta = (\beta'u_1)u_1 + (\beta'u_2)u_2$
- In addition $\alpha'u_1 = \beta' u_1$ and $\alpha'u_2 = -\beta' u_2$.  This characterizes the points on the hyperbola when $\gamma = \alpha$ or $\gamma = \beta$.

Most importantly, any $\gamma$ can be written as 

$g = w_1\left(\frac{\sqrt{2}}{2}(\alpha + \beta)'\right) + w_2\left(\frac{\sqrt{2}}{2}(\alpha - \beta)'\right) + \sqrt{1-w_1^2 - w_2^2} n$ where $n \in \text{Null}(\alpha, \beta)$ and $w_2 \in \left[-\sqrt{\frac{1-\alpha'\beta}{2}}, \sqrt{\frac{1-\alpha'\beta}{2}}\right]$ and $w_1 = \sqrt{\frac{1 + \frac{\alpha'\beta-1}{2}w_2^2}{\alpha'\beta + 1}}$


Below we plot the projection of the solution vectors onto the space spanned by $u_1, u_2$.  The black bold line represents valid solutions.  

```{r, echo=FALSE, fig.align="center"}
p <- 12

ab_list <- list()


alpha <- c(1, -10, 0, rep(0, p-3))
beta <- c(10, 0, 1, rep(0, p-3))
alpha <- alpha/sqrt(sum(alpha^2))
beta <- beta/sqrt(sum(beta^2))

ab_list[[2]] <- list(alpha=alpha, beta=beta)

alpha <- c(1, 0.4, 0, rep(0, p-3))
beta <- c(0.1, 4, 0, rep(0, p-3))
alpha <- alpha/sqrt(sum(alpha^2))
beta <- beta/sqrt(sum(beta^2))

ab_list[[1]] <- list(alpha=alpha, beta=beta)

alpha <- c(1, 0, rep(0, p-2))
beta <- c(0, 1, rep(0, p-2))

ab_list[[3]] <- list(alpha=alpha, beta=beta)

alpha <- c(1, 0.4, 0, rep(0, p-3))
beta <- -c(0.1, 4, 0, rep(0, p-3))
alpha <- alpha/sqrt(sum(alpha^2))
beta <- beta/sqrt(sum(beta^2))

ab_list[[4]] <- list(alpha=alpha, beta=beta)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))

par(mfrow=c(1, 4))
for(i  in 1:4) {

  alpha <- ab_list[[i]]$alpha
  beta <- ab_list[[i]]$beta
  
  m <- X %*% alpha
  e <- X %*% beta

  ## projection matrices into the nullspaces of alpha and beta
  
  M <- (alpha %*% t(beta) + beta %*% t(alpha))/2
  eig <- eigen(M)
  mvecs <- eig$vectors[, c(1,p)]
  mvals <- eig$values[c(1,p)]
  const <- t(alpha) %*% beta
  
  library(conics)
  xlim <- 1.1 * max(abs(t(cbind(alpha, beta)) %*%  mvecs[, 1]))
  ylim <- 1.1 * max(abs(t(cbind(alpha, beta)) %*%  mvecs[, 2]))
  conicPlot(c(mvals[1], 0, mvals[2], 0, 0, -const),
            ylim=c(-1, 1), xlim=c(-2, 2),
            xlab=expression(w[1]), ylab=expression(w[2]),
            main = sprintf("alpha'beta  = %.3f\n", t(alpha) %*% beta))
  
  if(const < 0) {   
    w_lim <- (t(alpha) %*% mvecs)[1]
    w_vec <- seq(w_lim, -w_lim, length.out=100)
    lines(w_vec, sqrt((c(t(alpha) %*% beta) - mvals[1] *   w_vec^2)/mvals[2]), lwd=3)
    points(t(cbind(alpha, beta)) %*%  mvecs[, 1],
           t(cbind(alpha, beta)) %*% mvecs[, 2], col="red", pch=19,cex=1.5)
    abline(h=0, lty=2)
    abline(v=0, lty=2)

  }
  else {
    browser()
    w_lim <- (t(alpha) %*% mvecs)[2]
    w_vec <- seq(-w_lim, w_lim, length.out=100)
    lines(sqrt((c(t(alpha) %*% beta) - mvals[1] * w_vec^2)/mvals[2]), w_vec, lwd=3)
    points(t(cbind(alpha, beta)) %*%  mvecs[, 1],
          t(cbind(alpha, beta)) %*% mvecs[, 2], col="red", pch=19,cex=1.5)
  abline(h=0, lty=2)
  abline(v=0, lty=2)
  
  text(t(cbind(alpha, beta)) %*%  mvecs[, 1],
       t(cbind(alpha, beta)) %*% mvecs[, 2],
       labels=c("alpha", "beta"), pos=2, offset=1, col="red", pch=19, cex=1.5)
  
  tst_vec <- sqrt(as.numeric(const / mvals[1])) * mvecs[, 1, drop=FALSE] +
    sqrt(as.numeric(1 - const/mvals[1])) * NullC(mvecs) %*% rustiefel(p-2, 1)
  points(t(tst_vec) %*%  mvecs[, 1], 0, pch=19, col="blue", cex=1.5)
  text(t(tst_vec) %*%  mvecs[, 1], -.1,
       pos=4, offset=2, label=sprintf("equiangular\n IP = %.3f", t(tst_vec) %*% alpha),
       pch=19, col="blue", srt=90)
  }

  


  

}
```





```{r, echo=FALSE, fig.align='center'}
p <- 12

alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

M <- (alpha %*% t(beta) + beta %*% t(alpha))/2
eig <- eigen(M)
mvecs <- eig$vectors[, c(1,p)]
mvals <- eig$values[c(1,p)]
const <- t(alpha) %*% beta

w2_lim <- (t(alpha) %*% mvecs)[2]
w2_vec <- seq(-w2_lim, w2_lim, length.out=100)

count <- 1
cor_m <- cor_e <- numeric(length(w2_vec))

for(w2 in seq(-w2_lim, w2_lim, length.out=100)) {
  
  w1 <- sqrt(as.numeric((t(alpha) %*% beta - mvals[2] * w2^2)/mvals[1]))
  
  gvec <- w1*mvecs[, 1] + w2 * mvecs[, 2] + sqrt(1-w1^2 - w2^2) * NullC(mvecs) %*% rustiefel(p-2, 1)
  
    ## step 5
  d <- Re(X %*% gvec)
  plot(d, e, xlim=c(-5, 5), ylim=c(-5, 5), pch=19, cex=0.2)
  cor_m[count] <- cor(m, d)
  cor_e[count] <- cor(e, d)
  count <- count + 1

}

plot(w2_vec, abs(cor_m), pch=19, col="red", 
     type="l", lwd=3, ylim=c(0, 1), ylab="Correlation with d(X)", xlab=expression(w[2]))
lines(w2_vec, abs(cor_e), pch=19, col="blue", lwd=3)
abline(h=abs(cor(e, m)), lty=2)
abline(v=0, lty=2)
legend("bottomright", legend=c("m(X)", "e(X)", "Cor(m,e)"), col=c("red", "blue", "black"), lty=c(1,1, 2))





```







### General solution space for $\gamma$ (deprecated? - 8/19)

Assume welog that $\Sigma = I$ and $\gamma$, $\alpha$ and $\beta$ are semi-orthogonal matrices.  Then the above condition is equivalent to
$$ \alpha'(I - \gamma\gamma^T)\beta = 0 $$

Let $M = (I - \gamma\gamma^T)$.  Then $\alpha M\beta' = 0$ implies that $\alpha M$ is in the null space of $\beta$ **or** $M\beta'$ is in the nullspace of $\alpha$.  For a $k$-dimensional reducer $\gamma$, this implies that $M$ is a rank ($p - k$) projection matrix into a subspace of the null spaces of $\alpha$ and/or $\beta$.  In short, the projections of $\alpha$ and $\beta$ onto the subspace are orthogonal.

Consider an example with $p=5$ and $k=1$.  If $\alpha$ and $\beta$ are both in $R^1$ then their null spaces span a three dimensional subspace of $R^4$.  If $M = N_\alpha = (I - \alpha\alpha')$ or $M = N_\beta = (I - \beta\beta')$ then clearly $\gamma=\alpha$ or $\gamma=\beta$ are solutions.  

<!-- More generally, we can write as $M = N_\alpha \bigoplus N_\beta$.   -->

The interesting solutions are the ones in which $M$ is a projection into a subspace of the nullspaces of both $\alpha$ *and* $\beta$.  For example, we can choose $M$ so that for any $s < p-1$, $s$ dimensions are in the span of the nullspace of $\alpha$ and the $p-s-1$ dimension are in the nullspace of $\beta$.

The most confusing part about this is the the repeated use of orthogonal complements.  First we work in the null space of alpha and beta, define a new subspace based on those, and then take the orthogonal complement to identify the subspace spanned for d(X).  The steps in this method, which yield a valid solution are:

1. Identify the null space of $\alpha$, $N_\alpha$
    + select a random set of coordinate directions $z$ so that $zz^T$ is a projection matrix into a subspace of $N_\alpha$
2. Identify the null space of $\beta$ outside the nullspace of $\alpha$: $N_{\beta/\alpha}$
    + select a random set of coordinate directions $w$ so that $ww^T$ is a projection into a subspace of $N_{\beta/\alpha}$
3. Define $M$ to be the projection matrix into $span(ww^T + zz^T)$ .  If $M$ is rank $p-k$ then $d(X)$ will be $k$-dimensional.
4. Let $g$ be the first $k$ eigenvectors of $I - M$
5. $d(X) = g'X$

I think any solution subspace can be represented in terms of the above procedure but haven't figured out a good way to  parameterize to control the correlation of $d(X)$ with $\alpha$ and $\beta$.

In this SNOTE we assume that $m_0$, $e$ and $d$ are all 1 dimensional.  

```{r}

p <- 12

alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta

# null space of alpha
A <- diag(p) - alpha %*% t(alpha)
# null space of beta
B <- diag(p) - beta %*% t(beta)

AnotB  <- eigen(A %*% (diag(p) - B))$vectors[, 1]
BnotA  <- eigen(B %*% (diag(p) - A))$vectors[, 1]

## this makes d perfectly correlated with alpha
#z <- cbind(AnotB, eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1:(p/2-1)])


## this makes d perfectly correlated with beta
#z <- eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1:p/2]

# A is a rank p-1 matrix, choose p/2 dimensional subspace of A
# choice p/2 is arbitrary but should balance alpha/beta contribution 
# this is step 1
z <- eigen(A)$vectors[, 1:(p-1)] %*% rustiefel(p-1, p/2)

# choose (p/2 - 1) dimensional subspace of B\A
# this is step 2
w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:(p/2-1)]

# Find the projection into the subspace spanned by z and w (step 3)
u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-1)]
M <- u %*% t(u)


## verify zero condition holds
alpha %*% M %*% beta
alpha %*% M %*% M %*% beta

## gamma is now the eigenvector of I - M (step 4)
eig <- eigen(diag(p) - M)

# should be exactly one eigenvalue equal to 1 (rest are 0)
eig$values

g <- eig$vectors[, 1]

## step 5
d <- Re(X %*% g)

print(sprintf("Corr(d, m) = %f", cor(m, d)))
print(sprintf("Corr(d, e) = %f", cor(e, d)))

mr <- lm(m ~ d-1)$residuals
er <- lm(e ~ d-1)$residuals
print(sprintf("Corr(m, e | d) = %f", cor(mr, er)))


```

### Exploring parameterization in terms of alpha/beta similarity with d(x)

TODO: Writeup discription and understand functional for the curve below

```{r}
p <- 12

alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)

alpha <- c(1, 0.4, 0, rep(0, p-3))
beta <- c(0.1, 4, 0, rep(0, p-3))
alpha <- alpha/sqrt(sum(alpha^2))
beta <- beta/sqrt(sum(beta^2))

alpha <- c(1, 0, rep(0, p-2))
beta <- c(0, 1, rep(0, p-2))

library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta

# null space of alpha
A <- diag(p) - alpha %*% t(alpha)
# null space of beta
B <- diag(p) - beta %*% t(beta)

AnotB  <- eigen(A %*% (diag(p) - B))$vectors[, 1]
BnotA  <- eigen(B %*% (diag(p) - A))$vectors[, 1]

n <- 100
ip <- cor_m <- cor_e <- numeric(n)
for(i in 1:n) {

  z <- cbind(AnotB, eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1]) %*% rustiefel(2, 1)
  w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:(p-2)]
  ip[i] <- t(z) %*% AnotB
  
  u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-1)]
  #u <- eigen(z %*% t(z))$vectors[, 1:(p-1)]
  M <- u %*% t(u)

  ## gamma is now the eigenvector of I - M (step 4)
  eig <- eigen(diag(p) - M)

  g <- eig$vectors[, 1]

  ## step 5
  d <- Re(X %*% g)

  cor_m[i] <- cor(m, d)
  cor_e[i] <- cor(e, d)

}

ord <- order(abs(ip))
plot(acos(abs(ip[ord])), abs(cor_m[ord]), pch=19, col="red", 
     type="l", lwd=3, ylim=c(0, 1), ylab="Correlation with d(X)", xlab="Alpha angle")
lines(acos(abs(ip[ord])), abs(cor_e[ord]), pch=19, col="blue", lwd=3)
abline(h=abs(cor(e, m)), lty=2)
legend("top", legend=c("m(X)", "e(X)", "Cor(m,e)"), col=c("red", "blue", "black"), lty=c(1,1, 2))

```


### Example where the dimension of $\alpha$ is greater than 1

WARNING: INCOMPLETE

In this example we assume 200 dimensional covariate space, $X$, and assume a high dimensional response surface $k=30$. 

- Look at example where dim($\gamma$) > 1
- Mixing 
- General reducer is highly correlated with the propensity score


```{r}
p <- 200
k <- 30

## choose a random value for alpha p x k
alpha <- rustiefel(p, k)

eps <- rnorm(p)
eps <- eps/sqrt(sum(eps^2))

## beta is 1d propensity score
beta <- (alpha %*% rustiefel(k, 1)) + eps
beta <- beta/sqrt(sum(beta^2))


library(mvtnorm)
library(rstiefel)

X <- rmvnorm(1e4, rep(0, p), diag(1, p))
m <- X %*% alpha
e <- X %*% beta

## projection matrices into the nullspaces of alpha and beta
A <- diag(p) - alpha %*% t(alpha)
B <- diag(p) - beta %*% t(beta)

## subspace for the nullspace of alpha and beta

## what are teh differences between these two?
z <- eigen(A)$vectors[, 1:((p-k)/2)]
w <- eigen(B - z %*% t(z))$vectors[, 1:((p-k)/2)]

u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-k)]
M <- u %*% t(u)

## verify zero condition holds
t(alpha) %*% M %*% beta

## gamma is now the eigenvector of I - M
eig <- eigen(diag(p) - M)
g <- eig$vectors[, 1:k]

# should be exactly one eigenvalue equal to 1 (rest are 0)
print(head(eig$values))

## verify
d <- X %*% g

## print(summary(cor(m, d)))
## print(summary(cor(m, e)))

## reducer is highly correlated with propensity score
## print(cor(e, d))
## print(cor(m, d))

print(cor(lm(e ~ m-1)$fitted.values, e))
print(cor(lm(e ~ m-1)$fitted.values, d))
mr <- lm(m ~ d-1)$residuals
er <- lm(e ~ d-1)$residuals

plot(mr[, 1], er)
plot(mr[, 5], er)

print(summary(cor(mr, er)))
summary(apply(mr, 2, function(mrCol) var(mrCol-er)))

```

