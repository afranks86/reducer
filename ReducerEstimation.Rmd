---
title: "ReducerEstimation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notebook for exploring reduction 

This notebook will explore _estimation_ in for reducer models.  For the setup and population see "ReducerNotes.Rmd".  

# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X \quad \text{(Expectation of control outcome)}\\
e(X) &= \text{logit}^{-1}(\mathbf{\beta}'X) \quad \text{(Propensity score)}\\
d(X) &= \gamma'X \quad \text{(Deconfounding score)}
\end{align}
$$

where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  

As a starting point, assume the true propensity scores are known and only the outcome model needs to be estiamted. 

```{r}
library(balanceHD)
library(mvtnorm)
library(rstiefel)
library(glmnet)


iters <- 25
true_ate_vec <- tau_hat_vec <- ipw_vec <- ipw_d_vec <- aipw_vec <- aipw_d_vec <- numeric(iters)
for(iter  in 1:iters) {
  print(iter)
  n = 500
  p = 1000
  
  alpha <- rustiefel(p, 1) 
  beta <- rustiefel(p, 1) 
  
  alpha <- c(1, -1, 0, rep(0, p-3))/sqrt(2)
  beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)
  cor(alpha, beta)
  
  X <- rmvnorm(n, rep(0, p), diag(1, p))
  mscale <- 15
  
  m <- mscale * X %*% alpha
  xb <- X %*% beta 
  escale <- 1/2
  
  e <- exp(escale*xb)/(1 + exp(escale*xb))
  hist(e)

  tau = 5
  T <- rbinom(n, 1, e)
  Y <- m +  tau * T + rnorm(n, 0, 1)
  
  # Y0 <- Y[T==0]
  # Y1 <- Y[T==1]
  # 
  # mean(Y0)
  # mean(Y0 / (1 - e[T==0]))
  # 
  # e_m <- predict(glm(T ~ m))
  # mean(T * Y / e_m) - mean((1-T) * Y / (1-e_m))
  # mean(T * Y / e) - mean((1-T) * Y / (1-e))
  
  
  Y_lambda_min <- cv.glmnet(cbind(T, X), Y, family="gaussian", 
                            alpha=0, penalty.factor = c(0, rep(1, p)), intercept=FALSE)$lambda.min
  glm_coefs <- coef(glmnet(cbind(T, X), Y, family="gaussian", 
                           alpha=0, penalty.factor = c(0, rep(1, p)),intercept=FALSE, 
                           lambda=Y_lambda_min))
  
  ehat <- predict(glmnet(X, T, family="binomial", alpha=0), newx=X, type="response")
  
  alpha_hat <- glm_coefs[-c(1:2)]
  tau_hat <- glm_coefs[2]    
  
  alpha_hat_norm <- sqrt(sum(alpha_hat^2))
  alpha_hat_normalized <- alpha_hat / alpha_hat_norm
  
  M <- (alpha_hat_normalized %*% t(beta) + beta %*% t(alpha_hat_normalized))/2
  eig <- eigen(M)
  mvecs <- eig$vectors[, c(1,p)]
  mvals <- eig$values[c(1,p)]
  if(abs(mvals[2]) > mvals[1]) {
    mvals <- -mvals[2:1]
    mvecs <- mvecs[, 2:1]
  }
  
  ab_dot_prod <- abs(t(alpha_hat_normalized) %*% beta)
  
  w2_lim <- (t(beta) %*% mvecs)[2]
  
  w2 <- 0*w2_lim
  w1 <- sqrt(as.numeric((ab_dot_prod - mvals[2] * w2^2)/mvals[1]))

  g <- w1*mvecs[, 1] + w2 * mvecs[, 2] + sqrt(1-w1^2 - w2^2) * NullC(mvecs) %*% rustiefel(p-2, 1)
  
  if(w2==0 & (abs(t(g) %*% beta) - abs(t(g) %*% alpha_hat_normalized) > 1e-10))
    browser()
  
  d <- Re(X %*% g)
  d_a <- X %*% alpha_hat_normalized
  
  res <- glm(T ~ d, family="binomial")
  hist(predict(res, type="response"))
  
  e_d <- predict(res, type="response")

  plt_indices <- sample(1:length(e), 30)
  shrinkage_plot(e[plt_indices], e_d[plt_indices])
  
  residual <- Y - X %*% alpha_hat - T * tau_hat
  plot((X %*% alpha_hat_normalized)[plt_indices], d[plt_indices]) 
 
  ate_ipw <- sum(1 / e[T==1] * Y[T==1]) / sum(1 / e[T==1]) -  sum(1 / (1-e[T==0]) * Y[T==0]) / sum(1 / (1-e[T==0]))
  
  bias <- get_bias(T=T, residual=Y, alpha_hat_normalized=alpha_hat_normalized, 
                  beta=beta, tau_hat=tau_hat, w2=0.97*w2_lim, times=1000)
  ate_ipw_d <- bias$bias1 - bias$bias0

  
  mu_treat <- mean(X %*% alpha_hat + tau_hat) + sum(T / e * residual) / sum(T / e)
  mu_ctrl <- mean(X %*% alpha_hat) + sum((1-T) / (1-e) * residual) / sum((1-T) / (1-e))
  
  ate_aipw <- mu_treat - mu_ctrl
  
  bias <- get_bias(T=T, residual=residual, alpha_hat_normalized=alpha_hat_normalized, 
                   beta=beta, tau_hat=tau_hat, w2=-0.95*w2_lim, times=1000)
  
  mu_treat <- mean(X %*% alpha_hat + tau_hat) + bias$bias1
  mu_ctrl <- mean(X %*% alpha_hat) + bias$bias0
  ate_aipw_d <- mu_treat - mu_ctrl
  

  print(true_ate)
  print(paste(tau_hat, ate_ipw, ate_ipw_d, ate_aipw, ate_aipw_d, sep=", "))
  
  true_ate_vec[iter] <- true_ate
  tau_hat_vec[iter] <- tau_hat
  ipw_vec[iter] <- ate_ipw
  ipw_d_vec[iter] <- ate_ipw_d
  aipw_vec[iter] <- ate_aipw
  aipw_d_vec[iter] <- ate_aipw_d
  
} 

# rmse
sqrt(apply((rbind(tau_hat_vec, ipw_vec, ipw_d_vec, aipw_vec, aipw_d_vec) - true_ate_vec)^2, 1, 
           function(x) mean(x, na.rm=TRUE)))

# mean absolute error
apply(abs(rbind(tau_hat_vec, ipw_vec, ipw_d_vec, aipw_vec, aipw_d_vec) - true_ate_vec), 1, 
      function(x) median(x, na.rm=TRUE))

# bias
apply(rbind(tau_hat_vec, ipw_vec, ipw_d_vec, aipw_vec, aipw_d_vec) - true_ate_vec, 1, 
      function(x) mean(x, na.rm=TRUE))


```




```{r}
logistic <- function(x) 1 / (1 + exp(-x))

## for different sample sizes from 50 to 250
bias_for_n <- c()
bias_for_n_a <- c()
bias_for_n_b <- c()

ubias_for_n <- c()
form_for_n <- c()

nvec <- round(seq(100, 500, length.out=10))
j <- 0
for(n in nvec) {

  j <- j + 1
  
  bias_vec_a <- bias_vec_b <- bias_vec <- c()
  raw_diff <- bias_form <- c()
  print(n)
  ## compute bias as average of 100 simulations
  for(i in 1:100) {
    
    p <- 50
    
    library(mvtnorm)
    library(rstiefel)
    library(glmnet)
    
    alpha <- rustiefel(p, 1) ## 4*c(1, -1, 0, rep(0, p-3))/sqrt(2)
    beta <- rustiefel(p, 1) ## c(1, 0, 1, rep(0, p-3))/sqrt(2)
    
    X <- rmvnorm(n, rep(0, p), diag(1, p))
    ## X <- matrix(2 * runif(p*n) - 1, nc=p)

    mscale <- 5
    m <- mscale * X %*% alpha
    
    xb <- X %*% beta 
    escale <- log(10)


    e <- exp(escale*xb)/(1 + exp(escale*xb))
    
    ## temporary hack
    ##e[e >= 1/2] <- 0.49
    ##e[e < -1/2] <- -0.49
    ## e <- jitter(e)
    
    T <- rbinom(n, 1, e)
    Y0 <- rnorm(n, m, 1)
    
    ## estimates of m and e
    ## beta_hat <- coef(lm((T - 1/2) ~ X - 1))
    
    
    beta_hat <- coef(glmnet(X, T, family="binomial"))
    
    T_lambda_min <- cv.glmnet(X, T, family="binomial", alpha=0, intercept=FALSE)$lambda.min
    beta_hat <- coef(glmnet(X, T, family="binomial", lambda=T_lambda_min, alpha=0, intercept=FALSE))[-1]
    
    # Y_lambda_min <- cv.glmnet(X[T==0, ], Y0[T==0], family="gaussian", alpha=0, intercept=FALSE)$lambda.min
    # alpha_hat <- coef(glmnet(X[T==0, ], Y0[T==0], family="gaussian", alpha=0, intercept=FALSE, lambda=Y_lambda_min))[-1]
    
    cor(beta, beta_hat)
    cor(alpha, alpha_hat)
    
    ## normalize to get d(X) surface
    alpha_hat_norm <- sqrt(sum(alpha_hat^2))
    beta_hat_norm <- sqrt(sum(beta_hat^2))
    alpha_hat_normalized <- alpha_hat / alpha_hat_norm
    beta_hat_normalized <- beta_hat / beta_hat_norm
    
    M <- (alpha %*% t(beta) + beta %*% t(alpha))/2
    eig <- eigen(M)
    mvecs <- eig$vectors[, c(1,p)]
    mvals <- eig$values[c(1,p)]
    const <- t(alpha) %*% beta
  
    w2 <- 0    
    w1 <- sqrt(as.numeric((t(alpha) %*% beta - mvals[2] * w2^2)/mvals[1]))
    
    g <- w1*mvecs[, 1] + w2 * mvecs[, 2] + sqrt(1-w1^2 - w2^2) * NullC(mvecs) %*% rustiefel(p-2, 1)

    d <- Re(X %*% g)
    d_a <- X %*% alpha_hat_normalized
    d_b <- X %*% beta_hat_normalized
    
    ## check uncorrelated condition holds
    res1 <- lm(d_a ~ d-1)$residuals
    res2 <- lm(d_b ~ d-1)$residuals
    
    print(sprintf("Residual correlation is %.3f", cor(res1, res2)))
    
    lm_md <- lm(m ~ d-1)
    mr <- lm_md$residuals
    mfit <- lm_md$fitted
    #lm_ed <- lm(e ~ d-1)
    lm_ed <- loess(e ~ d-1)
    er <- lm_ed$residuals
    efit <- lm_ed$fitted
    bias_vec[i] <- mean(cov(mr, er) / (1 - efit))
    
    if(j==1 & i==1) {    
      par(mfrow=c(1,2)) 
      hist(e, col="red")
      hist(efit, col="red")
    }
    
    mr_a <- lm(m ~ d_a -1)$residuals
    lm_ed_a <- loess(e ~ d_a-1)
    er_a <- lm_ed_a$residuals
    efit_a <- lm_ed_a$fitted
    bias_vec_a[i] <- mean(cov(mr_a, er_a) / (1 - efit_a))
    
    mr_b <- lm(m ~ d_b-1)$residuals
    #lm_ed_b <- lm(e ~ d_b-1)
    lm_ed_b <- loess(e ~ d_b-1)
    er_b <- lm_ed_b$residuals
    efit_b <- lm_ed_b$fitted
    bias_vec_b[i] <- mean(cov(mr_b, er_b) / (1 - efit_b))
    
    raw_diff[i] <- mean(Y0) - mean(Y0[T==0])
    bias_form[i] <- mean(cov(Y0, T) / (1-mean(e)))
    
  }
  bias_for_n[j] <- mean(bias_vec)  
  bias_for_n_a[j] <- mean(bias_vec_a)  
  bias_for_n_b[j] <- mean(bias_vec_b)  
  ubias_for_n[j] <- mean(raw_diff)
  form_for_n[j] <- mean(bias_form)
}
```

```{r}

unconditional_bias <- mean(as.numeric(cov(Y0, T))/(1-mean(e)))
print(unconditional_bias)

ymax = 1.1*max(abs(c(bias_for_n, bias_for_n_b, bias_for_n_a, form_for_n)))
# plot(nvec, bias_for_n, type="l", lwd=2, xlab="sample size", ylab="bias", ylim=c(-ymax, ymax), col="orange") 
plot(nvec, bias_for_n, type="l", lwd=2, xlab="sample size", ylab="bias", ylim=c(-.1, .1), col="orange") 
abline(h=0, lty=2)
lines(nvec, bias_for_n_b, type="l", lwd=2, xlab="sample size", ylab="bias", col="blue")
lines(nvec, bias_for_n_a, type="l", lwd=2, xlab="sample size", ylab="bias", col="red") 
lines(nvec, form_for_n, type="l", lwd=2, xlab="sample size", ylab="bias", lty=2)
lines(nvec, ubias_for_n, type="l", lwd=2, xlab="sample size", ylab="bias", lty=3)

legend("topright", legend=c("pi/4", "pi/2 (d=e)", "0 (d=m)"), col=c("black", "blue", "red"), lty=1, lwd=2, title="Alpha angle")



mean(Y0[T==1])
mean(Y0[T==0])
```

```{r, eval=FALSE}
## Under development. Not run.

normalize <- function(x){
  x / sqrt(sum(x^2))
}

n <- length(m)
zero_cov_obj <- function(gamma, X, m, e, lambda=0.01, lambda2=0.01, justmain=FALSE){
  d <- X %*% gamma
  d <- (d - mean(d)) / sd(d)
  ef <- loess(e ~ d)
  mf <- loess(m ~ d)
  ehat <- ef$fitted
  mhat <- mf$fitted
  main <- (mean(mhat * ehat) - mean(m * e))^2
  if(justmain) return(main)
  # Zero bias condition + unit norm condition
  sqrt(main) + sqrt((sqrt(sum(gamma^2)) - 1)^2) + 
    # Modulate agreement with prog vs pscore
    - lambda * sqrt(sum(mf$residuals^2)/n) - lambda2 * sqrt(sum(ef$residuals^2)/n)
}
ghat_a <- fit_ghat(alpha, X, m, e, lambda=0, lambda2=1)

fit_ghat <- function(init, X, m, e, lambda, lambda2){
  ghat_b <- optim(init,
                  function(g) zero_cov_obj(g, X, m, e, lambda, lambda2),
                  method="L-BFGS-B", control=list(trace=1))
  ghat_b$lambda <- lambda
  ghat_b$lambda2 <- lambda2
  class(ghat_b) <- "ghat"
  ghat_b
}

#ghat_b <- fit_ghat(alpha, X, m, e, lambda=1, lambda2=0)
#ghat_c <- fit_ghat(alpha, X, m, e, lambda=0.5, lambda2=0.5)

ghat_list <- lapply(seq(0, 1, by=0.1),
                    function(l2)
                      fit_ghat(beta, X, m, e, lambda=1-l2, lambda2=l2))

report <- function(x){
  UseMethod("report", x)
}  

report.ghat <- function(ghat){
  lambda <- ghat$lambda
  lambda2 <- ghat$lambda2
  print(zero_cov_obj(ghat$par, X, m, e, lambda, lambda2, justmain=TRUE))
  print(zero_cov_obj(ghat$par, X, m, e, lambda, lambda2, justmain=FALSE))
  gpar <- ghat$par
  print(cor(X%*%gpar, m))
  print(cor(X%*%gpar, e))
  print(gpar %>% round(3))
  return(gpar)
}

lapply(ghat_list, report)

alpha %>% normalize %>% round(3)
stop()
ghat_b <- optim(beta, function(g) zero_cov_obj(g, X, m, e), method="L-BFGS-B")

ghat_a$value
ghat_b$value


#ghat_g <- optim(Re(g), function(g) zero_cov_obj(g, X, m, e), method="L-BFGS-B")
#set.seed(0)
#ghat_r <- optim(rustiefel(p, 1), function(g) zero_cov_obj(g, X, m, e), method="L-BFGS-B")
cbind(ghat_a$par %>% normalize,
      (ghat_b$par * (-1 * (ghat_a$par[1] * ghat_b$par[1] < 0))) %>% normalize,
      alpha %>% normalize,
      beta %>% normalize) %>% matplot
```

