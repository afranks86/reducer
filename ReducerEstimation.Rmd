---
title: "ReducerEstimation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Notebook for exploring reduction 

This notebook will explore _estimation_ in for reducer models.  For the setup and population see "ReducerNotes.Rmd".  

# SNoTE
Consider both linear mean and propensity models. Assume that $r(X)$ is also linear in $X$.
$$
\begin{align}
m_0(X) &= \mathbf{\alpha}'X \quad \text{(Expectation of control outcome)}\\
e(X) &= \mathbf{\beta}'X \quad \text{(Propensity score)}\\
d(X) &= \gamma'X \quad \text{(Deconfounding score)}
\end{align}
$$

where $\alpha$ is an $p \times m_\alpha$ matrix and $\beta$ is a $p \times m_\beta$ matrix with $m < p$.  Assume (welog?) that $\alpha$ and $\beta$ are semi-orthogonal matrices, e.g. $\alpha^T\alpha = I$.  

```{r}

## for different sample sizes from 50 to 250
bias_for_n <- c()
bias_for_n_a <- c()
bias_for_n_b <- c()
for(j in 1:20) {
  n <-  50 + (j-1)*10
  bias_vec_a <- bias_vec_b <- bias_vec <- c()

    ## compute bias as average of 100 simulations
    for( i in 1:100) {
    
    p <- 12
    
    alpha <- 4*c(1, -1, 0, rep(0, p-3))/sqrt(2)
    beta <- c(1, 0, 1, rep(0, p-3))/sqrt(2)
    
    library(mvtnorm)
    library(rstiefel)
    
    X <- rmvnorm(n, rep(0, p), diag(.05, p))
    m <- X %*% alpha
    e <- X %*% beta 
    
    ## temporary hack
    e[e >= 1/2] <- 0.49
    e[e < -1/2] <- -0.49
    e <- jitter(e)
    
    T <- rbinom(n, 1, e + 1/2)
    Y0 <- rnorm(n, m, 1)
    
    ## estimates of m and e
    beta_hat <- coef(lm((T - 1/2) ~ X - 1))
    alpha_hat <- coef(lm(Y0[T==0] ~ X[T==0, ] - 1))
    
    cor(beta, beta_hat)
    cor(alpha, alpha_hat)
    
    ## normalize to get d(X) surface
    alpha_hat_norm <- sqrt(sum(alpha_hat^2))
    beta_hat_norm <- sqrt(sum(beta_hat^2))
    alpha_hat_normalized <- alpha_hat / alpha_hat_norm
    beta_hat_normalized <- beta_hat / beta_hat_norm
    
    # null space of alpha
    A <- diag(p) - alpha_hat_normalized %*% t(alpha_hat_normalized)
    # null space of beta
    B <- diag(p) - beta_hat_normalized %*% t(beta_hat_normalized)
    
    AnotB  <- eigen(A %*% (diag(p) - B))$vectors[, 1]
    BnotA  <- eigen(B %*% (diag(p) - A))$vectors[, 1]
    
    
    z <- cbind(AnotB, eigen(A %*% (diag(p) - AnotB %*% t(AnotB)))$vectors[, 1]) %*% c(cos(pi/4), sin(pi/4))
    w <- eigen(B %*% (diag(p) - z %*% t(z)))$vectors[, 1:(p-2)]
    
    u <- eigen(z %*% t(z) + w %*% t(w))$vectors[, 1:(p-1)]
    M <- u %*% t(u)
    
    alpha_hat %*% M %*% beta_hat
    
    ## gamma is now the eigenvector of I - M (step 4)
    eig <- eigen(diag(p) - M)
    
    g <- eig$vectors[, 1]
    
    ## step 5
    d <- Re(X %*% g)
    d_a <- X %*% alpha_hat
    d_b <- X %*% beta_hat
    
    mr <- lm(m ~ d-1)$residuals
    er <- lm(e ~ d-1)$residuals
    
    bias_vec[i] <- mean(cov(mr, er) / (1 - e))
    
    mr <- lm(m ~ d_a -1)$residuals
    er <- lm(e ~ d_a -1)$residuals
    
    bias_vec_a[i] <- mean(cov(mr, er) / (1 - e))
    
    mr <- lm(m ~ d_b-1)$residuals
    er <- lm(e ~ d_b-1)$residuals
    
    bias_vec_b[i] <- mean(cov(mr, er) / (1 - e))
    
  }
  bias_for_n[j] <- mean(bias_vec)  
  bias_for_n_a[j] <- mean(bias_vec_a)  
  bias_for_n_b[j] <- mean(bias_vec_b)  
}

unconditional_bias <- mean(as.numeric(cov(m, e))/(1-e))
print(unconditional_bias)
ymax <- 1.1 * unconditional_bias
plot(40 + (1:20)*10, bias_for_n, type="l", lwd=2, xlab="sample size", ylab="bias", ylim=c(0, ymax)) 
lines(40 + (1:20)*10, bias_for_n_b, type="l", lwd=2, xlab="sample size", ylab="bias", col="blue")
lines(40 + (1:20)*10, bias_for_n_a, type="l", lwd=2, xlab="sample size", ylab="bias", col="red") 
legend("topright", legend=c("pi/4", "pi/2 (d=e)", "0 (d=m)"), col=c("black", "blue", "red"), lty=1, lwd=2, title="Alpha angle")

abline(h=unconditional_bias, lty=2)


mean(Y0[T==1])
mean(Y0[T==0])
```

